import os
import re
import json
import unicodedata
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple

import numpy as np
import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings


# =========================
# CONFIG
# =========================

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# Gi·ªØ m·∫∑c ƒë·ªãnh gi·ªëng c·∫•u tr√∫c project hi·ªán t·∫°i c·ªßa b·∫°n, nh∆∞ng cho ph√©p override b·∫±ng env
CSV_PATH = os.getenv("HOTEL_CSV_PATH") or os.path.join(CURRENT_DIR, "..", "backend", "src", "data", "hotels.csv")
VECTOR_DB_PATH = os.getenv("VECTOR_DB_PATH") or os.path.join(CURRENT_DIR, "vectorstores", "db_faiss")

EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME", "sentence-transformers/all-MiniLM-L6-v2")
GEMINI_MODEL_NAME = os.getenv("GEMINI_MODEL_NAME", "gemini-2.5-flash")

os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY", "YOUR KEY")
# Hybrid weights (c√≥ th·ªÉ tinh ch·ªânh)
W_VEC = float(os.getenv("W_VEC", "0.50"))
W_LEX = float(os.getenv("W_LEX", "0.35"))
W_QUAL = float(os.getenv("W_QUAL", "0.15"))

# =========================
# TEXT NORMALIZATION
# =========================

def _strip_accents(text: str) -> str:
    if text is None:
        return ""
    text = str(text)
    text = unicodedata.normalize("NFD", text)
    text = "".join(ch for ch in text if unicodedata.category(ch) != "Mn")
    return unicodedata.normalize("NFC", text)


def _norm_text(text: str) -> str:
    s = _strip_accents(str(text).lower())
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = " ".join(s.split())
    return s


def _extract_star_from_row(star_val) -> Optional[int]:
    if pd.isna(star_val):
        return None
    s = str(star_val).lower()
    m = re.search(r"(\d+)", s)
    return int(m.group(1)) if m else None


def _extract_district_num(district_str) -> Optional[int]:
    """VD: 'Qu·∫≠n 5, ...' -> 5; 'District 1' -> 1; 'B√¨nh T√¢n' -> None"""
    if pd.isna(district_str):
        return None
    s = _strip_accents(str(district_str).lower())
    m = re.search(r"(qu·∫≠n|quan|district)\s*0?(\d+)", s)
    if m:
        return int(m.group(2))
    m2 = re.match(r"^\s*(\d+)\s*(?:,|$)", s)
    if m2:
        return int(m2.group(1))
    return None


def _district_norm(district_str) -> str:
    if district_str is None or (isinstance(district_str, float) and district_str != district_str):
        return ""
    s = str(district_str).split(",")[0]
    s = _norm_text(s)
    s = s.replace("district", "quan")
    return s


def _format_price_vnd(vnd: Optional[float]) -> str:
    """Format gi√° cho ng∆∞·ªùi d√πng. N·∫øu None -> 'ch∆∞a c·∫≠p nh·∫≠t gi√°'."""
    if vnd is None or (isinstance(vnd, float) and vnd != vnd):
        return "ch∆∞a c·∫≠p nh·∫≠t gi√°"
    try:
        p = float(vnd)
    except Exception:
        return "ch∆∞a c·∫≠p nh·∫≠t gi√°"
    if p <= 0:
        return "ch∆∞a c·∫≠p nh·∫≠t gi√°"

    p_int = int(round(p))

    # ∆Øu ti√™n d·∫°ng K / tri·ªáu cho d·ªÖ ƒë·ªçc
    if p_int >= 1_000_000:
        s = f"{p_int/1_000_000:.1f}".replace(".0", "")
        return f"{s} tri·ªáu VND/ƒë√™m"
    if p_int >= 1_000:
        return f"{p_int//1_000}K VND/ƒë√™m"

    return f"{p_int:,} VND/ƒë√™m"



# =========================
# PRICE INTENT / THRESHOLDS
# =========================

@dataclass
class PriceThresholds:
    q10: float
    q25: float
    q50: float
    q75: float
    q90: float


def _calc_price_thresholds(price_series: pd.Series) -> Optional[PriceThresholds]:
    p = pd.to_numeric(price_series, errors="coerce").dropna()
    if len(p) < 50:
        return None
    return PriceThresholds(
        q10=float(p.quantile(0.10)),
        q25=float(p.quantile(0.25)),
        q50=float(p.quantile(0.50)),
        q75=float(p.quantile(0.75)),
        q90=float(p.quantile(0.90)),
    )


def _price_bucket(price_vnd: Optional[float], thr: Optional[PriceThresholds]) -> str:
    if price_vnd is None or (isinstance(price_vnd, float) and price_vnd != price_vnd):
        return "chua_ro_gia"
    if thr is None:
        return "gia_thuong"
    if price_vnd <= thr.q25:
        return "gia_re"
    if price_vnd <= thr.q75:
        return "tam_trung"
    if price_vnd <= thr.q90:
        return "cao_cap"
    return "luxury"


def _has_explicit_price(query: str) -> bool:
    q = query.lower()
    return bool(re.search(r"\d+(?:[.,]\d+)?\s*tri·ªáu|\d+\s*(?:k|ngh√¨n|ngan|ngan)", q))


def _parse_constraints(query: str, thr: Optional[PriceThresholds]) -> Dict[str, Any]:
    """Parse ƒëi·ªÅu ki·ªán t·ª´ c√¢u h·ªèi + hi·ªÉu intent 'gi√° r·∫ª' theo ph√¢n ph·ªëi d·ªØ li·ªáu."""
    q_raw = query or ""
    q = _strip_accents(q_raw.lower())

    cons: Dict[str, Any] = {
        "min_price": None,
        "max_price": None,
        "district_nums": None,          # list[int]
        "district_names": None,         # list[str] (norm)
        "min_rating": None,
        "min_star": None,
        "sort_by": "relevance",
        "price_intent": None,           # gia_re / tam_trung / cao_cap / ...
        "explicit_price": False,
        "require_price": False,
    }

    # District number: qu·∫≠n 5 / quan 5 / district 5
    nums = set(int(m.group(2)) for m in re.finditer(r"(qu·∫≠n|quan|district)\s*(\d+)", q))
    if nums:
        cons["district_nums"] = sorted(nums)

    # District name (v√≠ d·ª•: binh thanh, go vap...)
    # N·∫øu query c√≥ '·ªü b√¨nh th·∫°nh' hay 'quan binh thanh' th√¨ b·∫Øt theo token.
    # (Danh s√°ch c·ª• th·ªÉ s·∫Ω ƒë∆∞·ª£c b·ªï sung ·ªü t·∫ßng filter d·ª±a tr√™n d·ªØ li·ªáu.)
    # ·ªû ƒë√¢y gi·ªØ raw ƒë·ªÉ t·∫ßng sau c√≥ th·ªÉ map.
    #
    # Star
    stars = []
    for m in re.finditer(r"(\d+)\s*sao", q):
        val = int(m.group(1))
        if 1 <= val <= 5:
            stars.append(val)
    if stars:
        cons["min_star"] = max(stars)

    # Rating
    rating_nums = []
    for m in re.finditer(r"(\d(?:[.,]\d)?)\s*/\s*5", q):
        rating_nums.append(float(m.group(1).replace(",", ".")))
    if rating_nums:
        cons["min_rating"] = max(rating_nums)
    else:
        for m in re.finditer(r"(tr√™n|tren|>=|l·ªõn h∆°n|lon hon)\s*(\d(?:[.,]\d)?)", q):
            num = float(m.group(2).replace(",", "."))
            if 0 <= num <= 5:
                cons["min_rating"] = max(cons["min_rating"] or 0, num)

    # Explicit price (tri·ªáu)
    def num_to_vnd(num_str: str) -> int:
        return int(float(num_str.replace(",", ".")) * 1_000_000)

    # 1-2 tri·ªáu
    m = re.search(r"(\d+(?:[.,]\d+)?)\s*[-‚Äì]\s*(\d+(?:[.,]\d+)?)\s*tri·ªáu", q)
    if m:
        cons["min_price"] = num_to_vnd(m.group(1))
        cons["max_price"] = num_to_vnd(m.group(2))
        cons["explicit_price"] = True

    # t·ª´ 1 tri·ªáu ƒë·∫øn 2 tri·ªáu
    m = re.search(r"t·ª´\s+(\d+(?:[.,]\d+)?)\s*tri·ªáu.*?(ƒë·∫øn|t·ªõi|-|toi)\s*(\d+(?:[.,]\d+)?)\s*tri·ªáu", q)
    if m:
        cons["min_price"] = num_to_vnd(m.group(1))
        cons["max_price"] = num_to_vnd(m.group(3))
        cons["explicit_price"] = True

    # d∆∞·ªõi 2 tri·ªáu
    m = re.search(r"(d∆∞·ªõi|duoi|nh·ªè h∆°n|nho hon|<=)\s*(\d+(?:[.,]\d+)?)\s*tri·ªáu", q)
    if m:
        cons["max_price"] = num_to_vnd(m.group(2))
        cons["explicit_price"] = True

    # tr√™n 1.5 tri·ªáu / t·ª´ 1.5 tri·ªáu
    m = re.search(r"(tr√™n|tren|t·ª´|tu|>=|l·ªõn h∆°n|lon hon)\s*(\d+(?:[.,]\d+)?)\s*tri·ªáu", q)
    if m:
        cons["min_price"] = num_to_vnd(m.group(2))
        cons["explicit_price"] = True

    # Qualitative price intent
    cheap_terms = ["gia re", "re", "binh dan", "tiet kiem", "economy", "budget", "khong dat"]
    very_cheap_terms = ["rat re", "re nhat", "sieu re"]
    mid_terms = ["tam trung", "hop ly", "vua tui", "vua phai"]
    high_terms = ["cao cap", "sang", "luxury", "5 sao", "sieu sang"]

    if not cons["explicit_price"]:
        if any(t in q for t in very_cheap_terms):
            cons["price_intent"] = "gia_re"
            cons["require_price"] = True
            if thr is not None:
                cons["max_price"] = int(thr.q10)
            cons["sort_by"] = "Gi√° tƒÉng d·∫ßn"
        elif any(t in q for t in cheap_terms):
            cons["price_intent"] = "gia_re"
            cons["require_price"] = True
            if thr is not None:
                cons["max_price"] = int(thr.q25)
            cons["sort_by"] = "Gi√° tƒÉng d·∫ßn"
        elif any(t in q for t in mid_terms):
            cons["price_intent"] = "tam_trung"
            if thr is not None:
                cons["min_price"] = int(thr.q25)
                cons["max_price"] = int(thr.q75)
        elif any(t in q for t in high_terms):
            cons["price_intent"] = "cao_cap"
            if thr is not None:
                cons["min_price"] = int(thr.q75)

    return cons


def _merge_constraints(base: Dict[str, Any], override: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    if not override:
        return base
    merged = dict(base)
    for k, v in override.items():
        if v not in (None, [], "", 0):
            merged[k] = v
    return merged


# =========================
# LOADERS
# =========================

def _detect_device() -> str:
    try:
        import torch  # type: ignore

        return "cuda" if torch.cuda.is_available() else "cpu"
    except Exception:
        return "cpu"


def load_llm() -> ChatGoogleGenerativeAI:
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p. H√£y export GOOGLE_API_KEY='YOUR_KEY'.")
    return ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME, temperature=0.0)


def load_vector_db() -> FAISS:
    if not os.path.exists(VECTOR_DB_PATH):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y vector DB ·ªü: {VECTOR_DB_PATH}. H√£y ch·∫°y prepare_vector_db_v2.py tr∆∞·ªõc.")
    device = _detect_device()
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={"device": device},
        encode_kwargs={"normalize_embeddings": True},
    )
    return FAISS.load_local(VECTOR_DB_PATH, embeddings, allow_dangerous_deserialization=True)


def load_hotel_dataframe() -> Tuple[pd.DataFrame, Optional[PriceThresholds]]:
    if not os.path.exists(CSV_PATH):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y CSV: {CSV_PATH}")
    df = pd.read_csv(CSV_PATH)
    df["_star_num"] = df["star"].apply(_extract_star_from_row)
    df["_district_num"] = df["district"].apply(_extract_district_num)
    df["_district_norm"] = df["district"].apply(_district_norm)
    df["hotelname_norm"] = df["hotelname"].astype(str).str.strip().str.lower()
    df["hotelname_norm_simple"] = df["hotelname"].apply(lambda x: _norm_text(re.sub(r"\b(khach san|kh√°ch s·∫°n|hotel)\b", " ", str(x))))
    df["_price_vnd"] = pd.to_numeric(df["price"], errors="coerce")
    thr = _calc_price_thresholds(df["_price_vnd"])
    return df, thr


# =========================
# LEXICAL (TF-IDF) RETRIEVER
# =========================

@dataclass
class LexicalIndex:
    vectorizer: TfidfVectorizer
    matrix: Any
    row_ids: np.ndarray


def build_lexical_index(df: pd.DataFrame, thr: Optional[PriceThresholds]) -> LexicalIndex:
    def row_text(row: pd.Series) -> str:
        price = row.get("_price_vnd")
        bucket = _price_bucket(price, thr)
        parts = [
            row.get("hotelname", ""),
            row.get("address", ""),
            row.get("district", ""),
            row.get("amenities", ""),
            row.get("description1", ""),
            row.get("reviews", ""),
            f"star {row.get('_star_num') or ''}",
            f"rating {row.get('totalScore') or ''}",
            f"price_bucket {bucket}",
        ]
        return _norm_text(" ".join(str(p) for p in parts if p))

    corpus = [row_text(r) for _, r in df.iterrows()]
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_features=200_000)
    matrix = vectorizer.fit_transform(corpus)
    row_ids = df.index.to_numpy()
    return LexicalIndex(vectorizer=vectorizer, matrix=matrix, row_ids=row_ids)


def lexical_topk(query: str, lex: LexicalIndex, k: int = 50) -> List[Tuple[int, float]]:
    qv = lex.vectorizer.transform([_norm_text(query)])
    sims = cosine_similarity(qv, lex.matrix).ravel()
    if k >= len(sims):
        top_idx = np.argsort(-sims)
    else:
        top_idx = np.argpartition(-sims, k)[:k]
        top_idx = top_idx[np.argsort(-sims[top_idx])]
    return [(int(lex.row_ids[i]), float(sims[i])) for i in top_idx if sims[i] > 0]


# =========================
# HOTEL FORMAT + FILTER
# =========================

def _row_to_hotel(row: pd.Series, match_reason: str = "") -> Dict[str, Any]:
    price = row.get("_price_vnd")
    rating = row.get("totalScore")
    star = row.get("_star_num")
    try:
        price = float(price) if price == price else None
    except Exception:
        price = None
    try:
        rating = float(rating) if rating == rating else None
    except Exception:
        rating = None
    try:
        star = int(star) if star == star else None
    except Exception:
        star = None

    return {
        "hotelname": row.get("hotelname") or "",
        "address": row.get("address") or "",
        "district": row.get("district"),
        "district_num": row.get("_district_num"),
        "rating": rating,
        "star": star,
        "price_vnd": price,
        "budget_vnd": price,  # backward-compat

        "price_text": _format_price_vnd(price),
        "url": row.get("url_google") or "",
        "image_url": row.get("imageUrl") or "",
        "amenities": row.get("amenities") or "",
        "description": row.get("description1") or "",
        "reviews": row.get("reviews") or "",
        "match_reason": match_reason,
    }


def _district_name_candidates(df: pd.DataFrame) -> Dict[str, str]:
    """map norm -> pretty string"""
    out: Dict[str, str] = {}
    for raw in df["district"].dropna().astype(str).unique().tolist():
        pretty = raw.split(",")[0].strip()
        out[_district_norm(raw)] = pretty
    return out


def _apply_constraints(df: pd.DataFrame, cons: Dict[str, Any]) -> pd.DataFrame:
    mask = pd.Series(True, index=df.index, dtype=bool)

    # District filter
    if cons.get("district_nums"):
        mask &= df["_district_num"].isin(cons["district_nums"])
    elif cons.get("district_names"):
        mask &= df["_district_norm"].isin(cons["district_names"])

    # Price filter
    if cons.get("min_price") is not None:
        mask &= df["_price_vnd"].notna() & (df["_price_vnd"] >= cons["min_price"])
    if cons.get("max_price") is not None:
        if cons.get("explicit_price") or cons.get("require_price"):
            # user c√≥ ƒëi·ªÅu ki·ªán gi√° (n√™u s·ªë ti·ªÅn ho·∫∑c intent 'gi√° r·∫ª') => lo·∫°i c√°c kh√°ch s·∫°n kh√¥ng c√≥ gi√°
            mask &= df["_price_vnd"].notna() & (df["_price_vnd"] <= cons["max_price"])
        else:
            # kh√¥ng c√≥ r√†ng bu·ªôc gi√° r√µ r√†ng => cho ph√©p gi√° NA
            mask &= (df["_price_vnd"].isna()) | (df["_price_vnd"] <= cons["max_price"])

    # N·∫øu user n√≥i "gi√° r·∫ª" nh∆∞ng kh√¥ng t√≠nh ƒë∆∞·ª£c ng∆∞·ª°ng gi√° (thr None) => v·∫´n y√™u c·∫ßu ph·∫£i c√≥ gi√°
    if cons.get("require_price") and cons.get("min_price") is None and cons.get("max_price") is None:
        mask &= df["_price_vnd"].notna()


    # Rating / star
    if cons.get("min_rating") is not None:
        mask &= pd.to_numeric(df["totalScore"], errors="coerce") >= cons["min_rating"]
    if cons.get("min_star") is not None:
        mask &= pd.to_numeric(df["_star_num"], errors="coerce") >= cons["min_star"]

    return df[mask].copy()


# =========================
# HYBRID RETRIEVAL + RANKING
# =========================

def _vec_topk(db: FAISS, query: str, k: int = 50) -> List[Tuple[str, float]]:
    """Return list of (hotelname, vec_sim) where vec_sim in (0,1]."""
    out: List[Tuple[str, float]] = []
    for doc, dist in db.similarity_search_with_score(query, k=k):
        meta = getattr(doc, "metadata", {}) or {}
        name = meta.get("hotelname") or ""
        if not name:
            continue
        # dist: lower is better. Convert to similarity.
        sim = 1.0 / (1.0 + float(dist))
        out.append((str(name), sim))
    return out


def _find_rows_by_names(df: pd.DataFrame, names: List[str]) -> Dict[str, int]:
    # map lower->index for quick lookup
    name_to_idx = {str(n).strip().lower(): int(i) for i, n in zip(df.index, df["hotelname"].astype(str))}
    out: Dict[str, int] = {}
    for nm in names:
        key = str(nm).strip().lower()
        if key in name_to_idx:
            out[nm] = name_to_idx[key]
    return out


def _quality_score(row: pd.Series) -> float:
    r = row.get("totalScore")
    s = row.get("_star_num")
    try:
        r = float(r) if r == r else 0.0
    except Exception:
        r = 0.0
    try:
        s = float(s) if s == s else 0.0
    except Exception:
        s = 0.0
    # 0..1
    return 0.7 * (r / 5.0) + 0.3 * (s / 5.0)


def _price_score(row: pd.Series, cons: Dict[str, Any], thr: Optional[PriceThresholds]) -> float:
    p = row.get("_price_vnd")
    if p is None or (isinstance(p, float) and p != p):
        # n·∫øu ch·ªâ n√≥i 'gi√° r·∫ª' th√¨ v·∫´n ƒë∆∞·ª£c ƒëi·ªÉm nh·ªè; n·∫øu user n√≥i r√µ s·ªë ti·ªÅn th√¨ 0.
        return 0.15 if (cons.get("max_price") is not None and not cons.get("explicit_price")) else 0.0
    p = float(p)

    # N·∫øu c√≥ range c·ª• th·ªÉ: ∆∞u ti√™n g·∫ßn gi·ªØa range
    if cons.get("min_price") is not None or cons.get("max_price") is not None:
        lo = float(cons.get("min_price") or p)
        hi = float(cons.get("max_price") or p)
        mid = (lo + hi) / 2.0
        # kho·∫£ng c√°ch chu·∫©n h√≥a
        denom = max(hi - lo, 1.0)
        d = abs(p - mid) / denom
        return float(max(0.0, 1.0 - d))

    # N·∫øu kh√¥ng c√≥ constraint c·ª• th·ªÉ, d√πng bucket
    b = _price_bucket(p, thr)
    if b == "gia_re":
        return 1.0
    if b == "tam_trung":
        return 0.6
    if b == "cao_cap":
        return 0.4
    if b == "luxury":
        return 0.25
    return 0.0


def _explain(row: pd.Series, cons: Dict[str, Any], thr: Optional[PriceThresholds], vec_sim: float, lex_sim: float) -> List[str]:
    lines: List[str] = []

    # District
    if cons.get("district_nums") and row.get("_district_num") in cons["district_nums"]:
        lines.append(f"ƒê√∫ng khu v·ª±c: Qu·∫≠n {int(row.get('_district_num'))}.")
    elif cons.get("district_names") and row.get("_district_norm") in cons["district_names"]:
        lines.append(f"ƒê√∫ng khu v·ª±c: {str(row.get('district')).split(',')[0].strip()}.")

    # Price
    p = row.get("_price_vnd")
    if p == p:
        p = float(p)
        if thr is not None and cons.get("price_intent") == "gia_re":
            lines.append(f"Gi√° {p/1_000_000:.1f} tri·ªáu/ƒë√™m ‚Äî thu·ªôc nh√≥m gi√° r·∫ª trong d·ªØ li·ªáu.")
        else:
            lines.append(f"Gi√° {p/1_000_000:.1f} tri·ªáu/ƒë√™m.")
    else:
        if cons.get("price_intent") == "gia_re":
            lines.append("Ch∆∞a c√≥ gi√°, nh∆∞ng v·∫´n g·ª£i √Ω th√™m ƒë·ªÉ b·∫°n tham kh·∫£o (c√≥ th·ªÉ h·ªèi l·∫°i gi√° khi ƒë·∫∑t).")

    # Rating / star
    r = row.get("totalScore")
    s = row.get("_star_num")
    try:
        r = float(r) if r == r else None
    except Exception:
        r = None
    try:
        s = int(s) if s == s else None
    except Exception:
        s = None
    if r is not None and s is not None:
        lines.append(f"Ch·∫•t l∆∞·ª£ng: {s} sao, rating {r:.1f}/5.")
    elif r is not None:
        lines.append(f"Rating {r:.1f}/5.")
    elif s is not None:
        lines.append(f"H·∫°ng {s} sao.")

    # Retrieval evidence
    if vec_sim > 0.0 and lex_sim > 0.0:
        lines.append("Kh·ªõp c·∫£ theo ng·ªØ nghƒ©a (vector) l·∫´n t·ª´ kh√≥a (BM25/TF-IDF).")
    elif vec_sim > 0.0:
        lines.append("Kh·ªõp m·∫°nh theo ng·ªØ nghƒ©a (vector).")
    elif lex_sim > 0.0:
        lines.append("Kh·ªõp m·∫°nh theo t·ª´ kh√≥a (TF-IDF).")

    return lines[:4]


def hybrid_search_hotels(
    user_query: str,
    df: pd.DataFrame,
    thr: Optional[PriceThresholds],
    vector_db: FAISS,
    lex: LexicalIndex,
    top_k: int = 5,
    filters: Optional[Dict[str, Any]] = None,
) -> List[Dict[str, Any]]:

    cons = _merge_constraints(_parse_constraints(user_query, thr), filters)

    # map district names from query against dataset if needed
    if not cons.get("district_nums"):
        name_map = _district_name_candidates(df)
        qn = _norm_text(user_query)
        hit_names = [norm for norm, pretty in name_map.items() if norm and norm in qn]
        if hit_names:
            cons["district_names"] = sorted(set(hit_names))

    # Candidate retrieval
    vec = _vec_topk(vector_db, user_query, k=60)
    lex_top = lexical_topk(user_query, lex, k=80)

    vec_names = [n for n, _ in vec]
    vec_name_to_sim: Dict[str, float] = {}
    for n, s in vec:
        vec_name_to_sim[n] = max(vec_name_to_sim.get(n, 0.0), s)

    # Build name->row index mapping for vector names
    name_to_idx = _find_rows_by_names(df, vec_names)

    # Collect candidates by row index
    cand: Dict[int, Dict[str, float]] = {}
    for nm, sim in vec_name_to_sim.items():
        idx = name_to_idx.get(nm)
        if idx is None:
            continue
        rec = cand.setdefault(int(idx), {})
        rec["vec"] = max(rec.get("vec", 0.0), float(sim))

    for idx, sim in lex_top:
        rec = cand.setdefault(int(idx), {})
        rec["lex"] = max(rec.get("lex", 0.0), float(sim))

    # Apply constraints
    df_cons = _apply_constraints(df, cons)
    allowed = set(df_cons.index.tolist())
    cand = {idx: sc for idx, sc in cand.items() if idx in allowed}

    # Fallback: n·∫øu candidate r·ªóng sau filter, l·∫•y tr·ª±c ti·∫øp t·ª´ df_cons theo rating/price
    if not cand:
        df_fb = df_cons.copy()
        df_fb["__rating"] = pd.to_numeric(df_fb["totalScore"], errors="coerce")
        df_fb["__star"] = pd.to_numeric(df_fb["_star_num"], errors="coerce")
        df_fb["__price"] = df_fb["_price_vnd"].fillna(10**12)
        df_fb = df_fb.sort_values(by=["__rating", "__star", "__price"], ascending=[False, False, True])
        out = []
        for _, row in df_fb.head(top_k).iterrows():
            h = _row_to_hotel(row, match_reason="Ph√π h·ª£p ti√™u ch√≠ l·ªçc (fallback)")
            h["explain"] = _explain(row, cons, thr, 0.0, 0.0)
            h["hybrid_score"] = None
            out.append(h)
        return out

    # Score + rank
    scored: List[Tuple[int, float, float, float, float]] = []
    for idx, sc in cand.items():
        row = df.loc[idx]
        vec_sim = float(sc.get("vec", 0.0))
        lex_sim = float(sc.get("lex", 0.0))
        qual = _quality_score(row)
        price_sc = _price_score(row, cons, thr)
        total = (W_VEC * vec_sim) + (W_LEX * lex_sim) + (W_QUAL * (0.7 * qual + 0.3 * price_sc))
        scored.append((idx, total, vec_sim, lex_sim, qual))

    scored.sort(key=lambda x: x[1], reverse=True)

    # Sort override
    sort_by = (filters or {}).get("sort_by") or cons.get("sort_by") or "relevance"

    # Build hotels
    out: List[Dict[str, Any]] = []
    for idx, total, vec_sim, lex_sim, qual in scored[: max(top_k * 3, top_k)]:
        row = df.loc[idx]
        h = _row_to_hotel(row, match_reason="Hybrid retrieval (vector + keyword)")
        h["hybrid_score"] = round(float(total), 6)
        h["vec_score"] = round(float(vec_sim), 6)
        h["lex_score"] = round(float(lex_sim), 6)
        h["explain"] = _explain(row, cons, thr, vec_sim, lex_sim)
        out.append(h)
        if len(out) >= top_k:
            break

    if sort_by == "Gi√° tƒÉng d·∫ßn":
        out.sort(key=lambda h: (h.get("price_vnd", h.get("budget_vnd")) is None, h.get("price_vnd", h.get("budget_vnd")) or 0))
    elif sort_by == "Gi√° gi·∫£m d·∫ßn":
        out.sort(key=lambda h: (h.get("price_vnd", h.get("budget_vnd")) is None, -(h.get("price_vnd", h.get("budget_vnd")) or 0)))
    elif sort_by == "Rating gi·∫£m d·∫ßn":
        out.sort(key=lambda h: (h.get("rating") is None, -(h.get("rating") or 0), -(h.get("star") or 0)))

    return out[:top_k]


# =========================
# ANSWER CHAIN (EXPLAINABLE)
# =========================

def build_answer_chain(llm: ChatGoogleGenerativeAI):
    template = """B·∫°n l√† Tr·ª£ l√Ω Du l·ªãch 3T2M1Stay ‚Äì tr·ª£ l√Ω t∆∞ v·∫•n l∆∞u tr√∫ am hi·ªÉu TP.HCM.
    B·∫°n lu√¥n tr·∫£ l·ªùi th√¢n thi·ªán, t·ª± nhi√™n, gi√†u c·∫£m x√∫c v·ª´a ph·∫£i (kh√¥ng s·∫øn), v√† h·ªØu √≠ch.

    D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO:
    - C√¢u h·ªèi: "{user_input}"
    - Danh s√°ch kh√°ch s·∫°n (JSON):
    {tool_result_json}

    NGUY√äN T·∫ÆC B·∫ÆT BU·ªòC (KH√îNG VI PH·∫†M):
    1) CH·ªà d√πng th√¥ng tin c√≥ trong JSON. Kh√¥ng b·ªãa th√™m kh√°ch s·∫°n/ƒë·ªãa ch·ªâ/th√¥ng tin.
    2) N·∫øu JSON r·ªóng: xin l·ªói ng·∫Øn g·ªçn + n√≥i r√µ kh√¥ng t√¨m th·∫•y theo ti√™u ch√≠ hi·ªán t·∫°i + g·ª£i √Ω 2‚Äì3 c√°ch n·ªõi ti√™u ch√≠.
    3) KH√îNG g·ª£i √Ω kh√°ch s·∫°n thi·∫øu t√™n ho·∫∑c thi·∫øu gi√°.
    - T√™n h·ª£p l·ªá: "hotelname" ho·∫∑c "name" kh√¥ng r·ªóng.
    - Gi√° h·ª£p l·ªá: "price_vnd" l√† s·ªë > 0. N·∫øu thi·∫øu gi√° ‚Üí lo·∫°i kh·ªèi g·ª£i √Ω.
    4) Kh√¥ng nh·∫Øc t·ªõi ‚ÄúJSON‚Äù, ‚Äútool‚Äù, ‚ÄúRAG‚Äù trong c√¢u tr·∫£ l·ªùi.

    C√ÅCH VI·∫æT (GI√öP VƒÇN PHONG PHONG PH√ö):
    - M·ªü ƒë·∫ßu 1‚Äì2 c√¢u: x√°c nh·∫≠n nhu c·∫ßu (khu v·ª±c + ti√™u ch√≠ gi√°).
    - M·ªói kh√°ch s·∫°n: 5‚Äì6 d√≤ng, di·ªÖn ƒë·∫°t t·ª± nhi√™n.
    - ‚ÄúV√¨ sao ph√π h·ª£p‚Äù: vi·∫øt th√†nh 2‚Äì3 g·∫°ch ƒë·∫ßu d√≤ng d·ª±a tr√™n "match_reason" ho·∫∑c "explain".
    - Th√™m 1 c√¢u ‚Äúg·ª£i √Ω nhanh‚Äù ph√π h·ª£p ƒë·ªëi t∆∞·ª£ng: ƒëi c√¥ng t√°c / c·∫∑p ƒë√¥i / ƒëi kh√°m b·ªánh / g·∫ßn ƒëi·ªÉm ti·ªán di chuy·ªÉn‚Ä¶ nh∆∞ng ph·∫£i suy ra h·ª£p l√Ω t·ª´ JSON (v√≠ d·ª•: qu·∫≠n, rating, star, m√¥ t·∫£), KH√îNG b·ªãa ƒë·ªãa danh.

    ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI:
    V·ªÄ L·ª∞A CH·ªåN T·ªêT NH·∫§T üèÜ
    - üè® T√™n:
    - üìç Qu·∫≠n/khu v·ª±c:
    - üí∞ Gi√°: {{price_text}}
    - ‚≠ê H·∫°ng/ƒë√°nh gi√°: (n·∫øu c√≥ th√¨ ghi; n·∫øu kh√¥ng c√≥ th√¨ b·ªè)
    - ‚ú® ƒêi·ªÉm n·ªïi b·∫≠t:
    ‚Ä¢ (√Ω 1 t·ª´ JSON)
    ‚Ä¢ (√Ω 2 t·ª´ JSON)
    ‚Ä¢ (√Ω 3 n·∫øu c√≥)
    - ‚úÖ Ph√π h·ª£p n·∫øu b·∫°n: (1 c√¢u ng·∫Øn)

    Bonus: C√ÅC G·ª¢I √ù ƒê√ÅNG C√ÇN NH·∫ÆC üí° (1‚Äì2 kh√°ch s·∫°n ti·∫øp theo)
    M·ªói kh√°ch s·∫°n 2‚Äì3 d√≤ng:
    - üè® T√™n ‚Äî üí∞ {{price_text}}
    ‚ú® 1 c√¢u m√¥ t·∫£ ƒëi·ªÉm m·∫°nh d·ª±a tr√™n JSON

    K·∫æT:
    - 1 c√¢u h·ªèi ch·ªët ƒë·ªÉ c√° nh√¢n ho√°: ng√¢n s√°ch t·ªëi ƒëa / ƒëi m·∫•y ng∆∞·ªùi / c·∫ßn g·∫ßn khu n√†o / ∆∞u ti√™n rating hay ph√≤ng r·ªông?

    B·∫Øt ƒë·∫ßu tr·∫£ l·ªùi:
    """
    prompt = ChatPromptTemplate.from_template(template)
    return prompt | llm | StrOutputParser()


# =========================
# TOOL-LIKE WRAPPER
# =========================

def search_hotels_tool(
    user_query: str,
    df: pd.DataFrame,
    thr: Optional[PriceThresholds],
    vector_db: FAISS,
    lex: LexicalIndex,
    top_k: int = 5,
    filters: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    hotels = hybrid_search_hotels(
        user_query=user_query,
        df=df,
        thr=thr,
        vector_db=vector_db,
        lex=lex,
        top_k=top_k,
        filters=filters,
    )
    return {"tool_name": "search_hotels_tool", "query": user_query, "results": hotels}


def chat_with_agent(
    user_input: str,
    llm: Optional[ChatGoogleGenerativeAI] = None,
    vector_db: Optional[FAISS] = None,
    df: Optional[pd.DataFrame] = None,
    thr: Optional[PriceThresholds] = None,
    lex: Optional[LexicalIndex] = None,
    filters: Optional[Dict[str, Any]] = None,
    top_k: int = 5,
) -> Dict[str, Any]:
    user_input = (user_input or "").strip()
    if not user_input:
        raise ValueError("user_input tr·ªëng ‚Äì h√£y nh·∫≠p c√¢u h·ªèi.")

    if df is None or thr is None:
        df, thr = load_hotel_dataframe()
    if vector_db is None:
        vector_db = load_vector_db()
    if lex is None:
        lex = build_lexical_index(df, thr)
    if llm is None:
        llm = load_llm()

    tool_result = search_hotels_tool(
        user_query=user_input,
        df=df,
        thr=thr,
        vector_db=vector_db,
        lex=lex,
        top_k=top_k,
        filters=filters,
    )

    answer_chain = build_answer_chain(llm)
    answer_text = answer_chain.invoke(
        {
            "user_input": user_input,
            "tool_result_json": json.dumps(tool_result, ensure_ascii=False, indent=2),
        }
    )
    return {"answer": answer_text, "tool_result": tool_result}


if __name__ == "__main__":
    llm = load_llm()
    vector_db = load_vector_db()
    df, thr = load_hotel_dataframe()
    lex = build_lexical_index(df, thr)

    while True:
        try:
            q = input("B·∫°n: ")
        except (EOFError, KeyboardInterrupt):
            break
        q = (q or "").strip()
        if not q:
            continue
        if q.lower() in {"exit", "quit"}:
            break

        result = chat_with_agent(q, llm=llm, vector_db=vector_db, df=df, thr=thr, lex=lex, top_k=5)
        print("Assistant:", result["answer"])
        print("-" * 60)
